The training uses Multiple Negatives Ranking Loss, which teaches the model to bring query–positive passage 
pairs closer in the embedding space while pushing away negatives. 
For each query, the positive passage is its ground-truth match, and all other passages in the batch act as negatives.
The loss is computed with cross-entropy over the similarity scores, encouraging the diagonal (true pairs) to be
much higher than the off-diagonal (false pairs). A lower loss means the model is better at distinguishing correct matches from incorrect ones.

Starting BERTIC SR training with query → abstract blocks...
 [1461/1461 1:06:55, Epoch 3/3]
Step	Training Loss
100	3.596300
200	2.976800
300	2.654700
400	2.473000
500	2.237000
600	1.876900
700	1.801300
800	1.720100
900	1.635700
1000	1.540800
1100	1.299100
1200	1.280600
1300	1.266500
1400	1.220600


*Second training

Starting BERTIC SR training with query → abstract blocks...
 [696/696 1:05:45, Epoch 3/3]
Step	Training Loss
100	0.746500
200	0.488000
300	0.294400
400	0.249700
500	0.192800
600	0.134500


Starting BERTIC SR training with query → abstract blocks...
 [696/696 1:09:15, Epoch 3/3]
Step	Training Loss
100	0.172200
200	0.177100
300	0.125300
400	0.098600
500	0.090800
600	0.065900
