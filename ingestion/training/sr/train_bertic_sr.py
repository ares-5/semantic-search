# Korpus Paralelnih Sažetaka doktorskih disertacija na srpskom i engleskom jeziku
# https://huggingface.co/datasets/jerteh/PaSaz

# Osim toga, kvalitet dobijenih rešenja znatno je premašio rezultate iz 
# literature
# 207425 - obrisi empty line

import os
from tqdm import tqdm
import torch
import pandas as pd
import shutil
from sentence_transformers import (
    SentenceTransformer, SentenceTransformerTrainer,
    SentenceTransformerTrainingArguments, InputExample, models, losses
)
from datasets import Dataset

def input_examples_to_dataset(input_examples):
    return Dataset.from_dict({
        "anchor": [ex.texts[0] for ex in input_examples],
        "positive": [ex.texts[1] for ex in input_examples]
    })

# CUDA setup
device = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(42)
print("Using device:", device)

# Load original PaSaz dataset (contains both en and sr abstracts)
dataset = pd.read_json("/kaggle/input/pasaz/pasaz.json", orient="records", lines=True)
df_small = dataset[["abstract_en", "abstract_sr", "full_abstract"]]
df_small = df_small[df_small["full_abstract"] == True]

# File path for synthetic queries generated by T5
out_file = "/kaggle/input/queries/generated_queries_all_sr.tsv"

# Load T5-generated synthetic queries (sr) and translate
train_examples = [] 
with open(out_file) as fIn:
    for line in fIn:
        try:
            query, paragraph = line.strip().split('\t', maxsplit=1)
            train_examples.append(InputExample(texts=[query, paragraph]))
        except:
            pass

print("Loaded training pairs (SR):", len(train_examples))

dataset = input_examples_to_dataset(train_examples)

# Tokenizer & BERTIC model
normalize = models.Normalize()
transformer = models.Transformer("classla/bcms-bertic", max_seq_length=512)
pooling = models.Pooling(transformer.get_word_embedding_dimension(), pooling_mode="mean")
model = SentenceTransformer(modules=[transformer, pooling, normalize]).to(device)

# Fine-tuning
loss_fn = losses.CachedMultipleNegativesRankingLoss(model, mini_batch_size=32)
num_epochs = 3

# Training args
training_args = SentenceTransformerTrainingArguments(
    output_dir="/kaggle/working/bertic-search-checkpoint",
    num_train_epochs=num_epochs,
    per_device_train_batch_size=32,
    logging_steps=100,
    save_strategy="epoch",
    remove_unused_columns=False,
    report_to="none"
)

# Trainer
trainer = SentenceTransformerTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    loss=loss_fn
)

# Train
print("Starting BERTIC SR training...")
trainer.train()

# Save final model
model.save("/kaggle/working/search_model_sr")
print("Final model saved to /kaggle/working/search_model_sr")
shutil.make_archive("search_model_sr", 'zip', "search_model_sr")
